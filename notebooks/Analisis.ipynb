{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis exploratorio de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Cargar el archivo CSV\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/comentarios_mejorados.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df = pd.read_csv('../data/comentarios_mejorados.csv')\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Obtener información sobre el DataFrame\n",
    "print(df.info())\n",
    "\n",
    "# Obtener estadísticas descriptivas del DataFrame\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Elimitar columnas que no aportan informacion\n",
    "En este caso se eliminan las columnas que no aportan informacion al modelo y nos quedamos con el comentario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_a_eliminar = ['channelId', 'videoId', 'textOriginal', 'authorDisplayName', \n",
    "                       'authorProfileImageUrl', 'authorChannelUrl', 'authorChannelId', \n",
    "                       'canRate', 'viewerRating', 'likeCount', 'publishedAt', \n",
    "                       'updatedAt', 'parentId']\n",
    "\n",
    "# Eliminar las columnas\n",
    "df = df.drop(columnas_a_eliminar, axis=1)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Limpieza de texto\n",
    "Se eliminan los caracteres especiales, se pasa todo a minusculas y se eliminan espaacios en blanco innecesarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Función para limpiar el texto\n",
    "def clean_text(text):\n",
    "    # Eliminar caracteres especiales y emojis\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text)\n",
    "    # Minúsculas\n",
    "    text = text.lower()\n",
    "    # Eliminar espacios en blanco innecesarios\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Aplicar la función de limpieza de texto a la columna 'textDisplay'\n",
    "df['textDisplay'] = df['textDisplay'].apply(clean_text)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizacion\n",
    "Se separa el texto en palabras individuales. Esto nos permite analizar cada palabra por separado y no como una cadena de texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Descargar el paquete 'punkt' de NLTK solo es necesario la primera vez\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenizar el texto\n",
    "df['textDisplay'] = df['textDisplay'].apply(nltk.word_tokenize)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Eliminacion de stopwords\n",
    "Se eliminan las palabras que no aportan informacion al modelo. Estas palabras son las que se encuentran en la lista de stopwords como por ejemplo: \"el\", \"la\", \"de\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar las palabras vacías\n",
    "from nltk.corpus import stopwords\n",
    "# Descargar las palabras vacías de NLTK solo es necesario la primera vez\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('spanish')\n",
    "\n",
    "# Función para eliminar las palabras vacías\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "# Eliminar las palabras vacías\n",
    "df['textDisplay'] = df['textDisplay'].apply(remove_stopwords)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stemming o lematizacion\n",
    "Se eliminan los sufijos de las palabras para quedarnos con la raiz de la palabra. Esto nos permite reducir el numero de palabras que se tienen que analizar. Por ejemplo: \"corriendo\" se convierte en \"correr\". \n",
    "\n",
    "Esto es útil en análisis de sentimientos ya que la raíz de la palabra puede ser más importante que la palabra en sí. Por ejemplo, \"bueno\" y \"mejor\" tienen la misma raíz, \"buen\", por lo que se puede decir que son similares.\n",
    "\n",
    "Para lematizar se utiliza la libreria de Spacy, ya que ntltk no tiene soporte para el idioma español.\n",
    "\n",
    "En nuestro caso instalaremos una versión precompilada de Spacy para el idioma español, ya que la compilación de la libreria puede tardar mucho tiempo.\n",
    "\n",
    "\" pip install https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.0/es_core_news_sm-2.2.0.tar.gz \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargar el modelo de lenguaje español de Spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Función para lematizar el texto\n",
    "def lemmatization(texts):\n",
    "    output = []\n",
    "    for text in texts:\n",
    "        doc = nlp(\" \".join(text)) \n",
    "        output.append([token.lemma_ for token in doc])\n",
    "    return output\n",
    "\n",
    "# Lematizar el texto\n",
    "df['textDisplay'] = lemmatization(df['textDisplay'])\n",
    "\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
