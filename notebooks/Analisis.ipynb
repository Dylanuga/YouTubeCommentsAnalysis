{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis exploratorio de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(850, 14)\n",
      "                  channelId      videoId  \\\n",
      "0  UCmb0LnmFYceH7toqgmUTJDA  U5wTE1t9MqU   \n",
      "1  UCmb0LnmFYceH7toqgmUTJDA  U5wTE1t9MqU   \n",
      "2  UCmb0LnmFYceH7toqgmUTJDA  U5wTE1t9MqU   \n",
      "3  UCmb0LnmFYceH7toqgmUTJDA  U5wTE1t9MqU   \n",
      "4  UCmb0LnmFYceH7toqgmUTJDA  U5wTE1t9MqU   \n",
      "\n",
      "                                         textDisplay  \\\n",
      "0       Crack es en ps5 las cinemática o ps4 también   \n",
      "1  liga femenina , será igual de mala que en la v...   \n",
      "2                                  Juego malisimo...   \n",
      "3  A mi me lo regalaron y los graficos justitos p...   \n",
      "4  Vaya basura de video como se nota que le han p...   \n",
      "\n",
      "                                        textOriginal      authorDisplayName  \\\n",
      "0       Crack es en ps5 las cinemática o ps4 también  @elreylobodepatio4960   \n",
      "1  liga femenina , será igual de mala que en la v...             @yemon5329   \n",
      "2                                  Juego malisimo...       @user-so2tt2wx8n   \n",
      "3  A mi me lo regalaron y los graficos justitos p...         @sergimateu792   \n",
      "4  Vaya basura de video como se nota que le han p...       @tonyesanchez820   \n",
      "\n",
      "                               authorProfileImageUrl  \\\n",
      "0  https://yt3.ggpht.com/ytc/AIf8zZSqF7EPiQXR0sws...   \n",
      "1  https://yt3.ggpht.com/ytc/AIf8zZRS0xx4TgtD4OEc...   \n",
      "2  https://yt3.ggpht.com/8v48UN-YmRB1f4qHY4m2yl5O...   \n",
      "3  https://yt3.ggpht.com/ytc/AIf8zZQGf6GckvEUNIo1...   \n",
      "4  https://yt3.ggpht.com/ytc/AIf8zZSLB_Qk1wbrGvAz...   \n",
      "\n",
      "                                    authorChannelUrl  \\\n",
      "0  http://www.youtube.com/channel/UC_j9t8kNHO9I3O...   \n",
      "1  http://www.youtube.com/channel/UCSSxtC6mrp10lV...   \n",
      "2  http://www.youtube.com/channel/UCcfS1qhzdEhkAJ...   \n",
      "3  http://www.youtube.com/channel/UCwHWjVUofVeE_h...   \n",
      "4  http://www.youtube.com/channel/UCXfnHOR6jp48ov...   \n",
      "\n",
      "                         authorChannelId  canRate viewerRating  likeCount  \\\n",
      "0  {'value': 'UC_j9t8kNHO9I3OFZziz0xbw'}     True         none          0   \n",
      "1  {'value': 'UCSSxtC6mrp10lVV9PIT9gvQ'}     True         none          0   \n",
      "2  {'value': 'UCcfS1qhzdEhkAJF9C8LYWbw'}     True         none          0   \n",
      "3  {'value': 'UCwHWjVUofVeE_hjhSzO8NfQ'}     True         none          0   \n",
      "4  {'value': 'UCXfnHOR6jp48ovkCEmwq6Xw'}     True         none          0   \n",
      "\n",
      "            publishedAt             updatedAt parentId  \n",
      "0  2024-01-10T05:10:16Z  2024-01-10T05:10:16Z      NaN  \n",
      "1  2024-01-09T08:30:32Z  2024-01-09T08:30:32Z      NaN  \n",
      "2  2024-01-08T16:23:13Z  2024-01-08T16:23:13Z      NaN  \n",
      "3  2024-01-04T15:41:58Z  2024-01-04T15:41:58Z      NaN  \n",
      "4  2024-01-04T00:34:58Z  2024-01-04T00:34:58Z      NaN  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 850 entries, 0 to 849\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   channelId              850 non-null    object\n",
      " 1   videoId                522 non-null    object\n",
      " 2   textDisplay            850 non-null    object\n",
      " 3   textOriginal           850 non-null    object\n",
      " 4   authorDisplayName      850 non-null    object\n",
      " 5   authorProfileImageUrl  850 non-null    object\n",
      " 6   authorChannelUrl       850 non-null    object\n",
      " 7   authorChannelId        850 non-null    object\n",
      " 8   canRate                850 non-null    bool  \n",
      " 9   viewerRating           850 non-null    object\n",
      " 10  likeCount              850 non-null    int64 \n",
      " 11  publishedAt            850 non-null    object\n",
      " 12  updatedAt              850 non-null    object\n",
      " 13  parentId               328 non-null    object\n",
      "dtypes: bool(1), int64(1), object(12)\n",
      "memory usage: 87.3+ KB\n",
      "None\n",
      "        likeCount\n",
      "count  850.000000\n",
      "mean     4.091765\n",
      "std     18.929627\n",
      "min      0.000000\n",
      "25%      0.000000\n",
      "50%      0.000000\n",
      "75%      2.000000\n",
      "max    302.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df = pd.read_csv('../data/comentarios_mejorados.csv', sep='|')\n",
    "\n",
    "# Mostrar las filas y columnas del DataFrame\n",
    "print(df.shape)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Obtener información sobre el DataFrame\n",
    "print(df.info())\n",
    "\n",
    "# Obtener estadísticas descriptivas del DataFrame\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Elimitar columnas que no aportan informacion\n",
    "En este caso se eliminan las columnas que no aportan informacion al modelo y nos quedamos con el comentario. Eliminamos también los comentarios que son respuestas a otros comentarios basandonos en el hecho de que no aportan información al modelo.\n",
    "\n",
    "Para ello nos fijamos en el parent_id, si es nulo es un comentario raiz, si no es una respuesta a otro comentario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522\n",
      "parentId\n",
      "True     522\n",
      "False    328\n",
      "Name: count, dtype: int64\n",
      "(522, 14)\n"
     ]
    }
   ],
   "source": [
    "# Contamos las filas donde el parent_id sea nulo\n",
    "print(df.parentId.isnull().sum())\n",
    "print(df.parentId.isnull().value_counts())\n",
    "\n",
    "# Filtrar las filas donde el parent_id sea nulo\n",
    "df = df[df.parentId.isnull()]\n",
    "\n",
    "# Contamos las filas que quedaron\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         textDisplay\n",
      "0       Crack es en ps5 las cinemática o ps4 también\n",
      "1  liga femenina , será igual de mala que en la v...\n",
      "2                                  Juego malisimo...\n",
      "3  A mi me lo regalaron y los graficos justitos p...\n",
      "4  Vaya basura de video como se nota que le han p...\n"
     ]
    }
   ],
   "source": [
    "columnas_a_eliminar = ['channelId', 'videoId', 'textOriginal', 'authorDisplayName', \n",
    "                       'authorProfileImageUrl', 'authorChannelUrl', 'authorChannelId', \n",
    "                       'canRate', 'viewerRating', 'likeCount', 'publishedAt', \n",
    "                       'updatedAt', 'parentId']\n",
    "\n",
    "# Eliminar las columnas\n",
    "df = df.drop(columnas_a_eliminar, axis=1)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Limpieza de texto\n",
    "Se eliminan los caracteres especiales, se pasa todo a minusculas y se eliminan espaacios en blanco innecesarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         textDisplay\n",
      "0       crack es en ps5 las cinemática o ps4 también\n",
      "1  liga femenina será igual de mala que en la vid...\n",
      "2                                     juego malisimo\n",
      "3  a mi me lo regalaron y los graficos justitos p...\n",
      "4  vaya basura de video como se nota que le han p...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Función para limpiar el texto\n",
    "def clean_text(text):\n",
    "    # Eliminar emojis y caracteres especiales manteniendo las tildes y ñ\n",
    "    text = re.sub(r'([^\\s\\wñáéíóú]|_)+', '', text)\n",
    "    \n",
    "    # Minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Eliminar espacios en blanco innecesarios\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Aplicar la función de limpieza de texto a la columna 'textDisplay'\n",
    "df['textDisplay'] = df['textDisplay'].apply(clean_text)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizacion\n",
    "Se separa el texto en palabras individuales. Esto nos permite analizar cada palabra por separado y no como una cadena de texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         textDisplay\n",
      "0  [crack, es, en, ps5, las, cinemática, o, ps4, ...\n",
      "1  [liga, femenina, será, igual, de, mala, que, e...\n",
      "2                                  [juego, malisimo]\n",
      "3  [a, mi, me, lo, regalaron, y, los, graficos, j...\n",
      "4  [vaya, basura, de, video, como, se, nota, que,...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dylan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Descargar el paquete 'punkt' de NLTK solo es necesario la primera vez\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenizar el texto\n",
    "df['textDisplay'] = df['textDisplay'].apply(nltk.word_tokenize)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Eliminacion de stopwords\n",
    "Se eliminan las palabras que no aportan informacion al modelo. Estas palabras son las que se encuentran en la lista de stopwords como por ejemplo: \"el\", \"la\", \"de\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         textDisplay\n",
      "0                      [crack, ps5, cinemática, ps4]\n",
      "1          [liga, femenina, igual, mala, vida, real]\n",
      "2                                  [juego, malisimo]\n",
      "3  [regalaron, graficos, justitos, jugabilidad, m...\n",
      "4  [vaya, basura, video, nota, pagado, blanquee, ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dylan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Eliminar las palabras vacías\n",
    "from nltk.corpus import stopwords\n",
    "# Descargar las palabras vacías de NLTK solo es necesario la primera vez\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('spanish')\n",
    "\n",
    "# Función para eliminar las palabras vacías\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "# Eliminar las palabras vacías\n",
    "df['textDisplay'] = df['textDisplay'].apply(remove_stopwords)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stemming o lematizacion\n",
    "Se eliminan los sufijos de las palabras para quedarnos con la raiz de la palabra. Esto nos permite reducir el numero de palabras que se tienen que analizar. Por ejemplo: \"corriendo\" se convierte en \"correr\". \n",
    "\n",
    "Esto es útil en análisis de sentimientos ya que la raíz de la palabra puede ser más importante que la palabra en sí. Por ejemplo, \"bueno\" y \"mejor\" tienen la misma raíz, \"buen\", por lo que se puede decir que son similares.\n",
    "\n",
    "Para lematizar se utiliza la libreria de Spacy, ya que ntltk no tiene soporte para el idioma español.\n",
    "\n",
    "En nuestro caso instalaremos una versión precompilada de Spacy para el idioma español, ya que la compilación de la libreria puede tardar mucho tiempo.\n",
    "\n",
    "\" pip install https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.0/es_core_news_sm-2.2.0.tar.gz \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words changed after lemmatization: 2594\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargar el modelo de lenguaje español de Spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Función para lematizar el texto\n",
    "def lemmatization(texts):\n",
    "    output = []\n",
    "    changed_words_count = 0\n",
    "    for text in texts:\n",
    "        # Crear una lista de palabras antes de la lematización\n",
    "        words_before = text\n",
    "        \n",
    "        # Lematizar el texto\n",
    "        doc = nlp(\" \".join(words_before)) \n",
    "        words_after = [token.lemma_ for token in doc]\n",
    "        \n",
    "        # Comparar las palabras antes y después de la lematización\n",
    "        for before, after in zip(words_before, words_after):\n",
    "            if before != after:\n",
    "                changed_words_count += 1\n",
    "        \n",
    "        output.append(words_after)\n",
    "    \n",
    "    print(f\"Numero de palabras que han cambiado después de la lemantización: {changed_words_count}\")\n",
    "    return output\n",
    "\n",
    "# Lematizar el texto\n",
    "df['textDisplay'] = lemmatization(df['textDisplay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargar el modelo de lenguaje español de Spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Lematizar la palabra \"corriendo\"\n",
    "doc = nlp(\"corriendo\")\n",
    "for token in doc:\n",
    "    print(token.lemma_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
